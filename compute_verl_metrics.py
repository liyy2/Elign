import argparse
import json
import math
import os
import pickle
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import torch
from tqdm import tqdm
from omegaconf import OmegaConf
from tensordict import TensorDict
import sys
sys.path.append("/home/yl2428/e3_diffusion_for_molecules-main/edm_source")
from edm_source.configs.datasets_config import get_dataset_info
from verl_diffusion.protocol import DataProto
from verl_diffusion.worker.reward.force import UMAForceReward
from verl_diffusion.utils.rdkit_metrics import compute_rdkit_metrics


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Compute UMA-based reward metrics for molecules generated by eval_verl_rollout.py. "
            "Loads the saved samples, rebuilds DataProto batches, and reports aggregate statistics."
        )
    )
    parser.add_argument(
        "--run-dir",
        type=str,
        required=True,
        help="Directory containing VERL training artifacts (config.yaml, args.pickle, checkpoints, samples).",
    )
    parser.add_argument(
        "--samples",
        type=str,
        default=None,
        help="Path to the generated samples file (torch.save). Defaults to <run-dir>/eval_rollouts.pt.",
    )
    parser.add_argument(
        "--args-pickle",
        type=str,
        default=None,
        help="Path to the EDM args.pickle file. Defaults to <run-dir>/args.pickle.",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=256,
        help="Number of samples per UMA evaluation batch.",
    )
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="Device for UMAForceReward (cpu, cuda, cuda:0, ...). Defaults to CUDA if available.",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="Optional path to write aggregate metrics (JSON).",
    )
    parser.add_argument(
        "--save-raw",
        type=str,
        default=None,
        help="Optional path to torch.save the per-sample metric tensors.",
    )
    return parser.parse_args()


def make_absolute(path_value: Optional[str], base_dir: Path) -> Optional[Path]:
    if path_value is None:
        return None
    path = Path(path_value)
    if not path.is_absolute():
        path = (base_dir / path).resolve()
    return path


def load_edm_config(args_pickle: Path) -> Any:
    with open(args_pickle, "rb") as f:
        edm_config = pickle.load(f)
    if isinstance(edm_config, dict):
        edm_config = OmegaConf.create(edm_config)
    if hasattr(edm_config, "datadir") and edm_config.datadir is None:
        edm_config.datadir = "qm9/temp"
    if not hasattr(edm_config, "normalization_factor"):
        edm_config.normalization_factor = 1
    if not hasattr(edm_config, "aggregation_method"):
        edm_config.aggregation_method = "sum"
    return edm_config


def load_run_config(run_dir: Path) -> Dict[str, Any]:
    config_path = run_dir / "config.yaml"
    if not config_path.exists():
        return {}
    import yaml

    with open(config_path, "r") as f:
        data = yaml.safe_load(f) or {}
    return data


def select_device(device_arg: Optional[str]) -> torch.device:
    if device_arg:
        device_arg = str(device_arg)
        if device_arg.lower() == "cuda":
            device_arg = "cuda:0"
        return torch.device(device_arg)
    if torch.cuda.is_available():
        return torch.device("cuda:0")
    return torch.device("cpu")


def load_samples(samples_path: Path) -> Tuple[List[Dict[str, torch.Tensor]], Dict[str, Any]]:
    payload = torch.load(samples_path, map_location="cpu")
    metadata: Dict[str, Any] = {}
    if isinstance(payload, dict) and "samples" in payload:
        samples = payload["samples"]
        metadata = {k: v for k, v in payload.items() if k != "samples"}
    elif isinstance(payload, list):
        samples = payload
    else:
        raise ValueError(f"Unrecognized sample payload structure in {samples_path}")
    if not isinstance(samples, list) or not samples:
        raise ValueError(f"No samples found in {samples_path}")
    return samples, metadata


def build_dataproto(
    batch_samples: Iterable[Dict[str, Any]],
    dataset_info: Dict[str, Any],
) -> DataProto:
    batch_list = list(batch_samples)
    if not batch_list:
        raise ValueError("Empty batch passed to build_dataproto")

    max_n_nodes = int(dataset_info["max_n_nodes"])
    num_atom_types = len(dataset_info["atom_decoder"])

    x_tensors: List[torch.Tensor] = []
    cat_tensors: List[torch.Tensor] = []
    nodes_counts: List[int] = []
    group_indices: List[int] = []
    timesteps: List[torch.Tensor] = []
    has_group_index = True
    has_timesteps = True

    for sample in batch_list:
        positions = sample["positions"]
        if not isinstance(positions, torch.Tensor):
            positions = torch.tensor(positions, dtype=torch.float32)
        positions = positions.to(torch.float32)

        atom_types = sample.get("atom_types")
        if atom_types is None:
            raise ValueError("Sample missing 'atom_types'")
        if not isinstance(atom_types, torch.Tensor):
            atom_types = torch.tensor(atom_types, dtype=torch.long)
        atom_types = atom_types.to(torch.long)

        num_atoms = int(sample.get("num_atoms", atom_types.shape[0]))
        num_atoms = max(0, min(num_atoms, max_n_nodes))

        x_pad = torch.zeros(max_n_nodes, 3, dtype=torch.float32)
        x_pad[:num_atoms] = positions[:num_atoms]

        categorical = torch.zeros(max_n_nodes, num_atom_types, dtype=torch.float32)
        if num_atoms > 0:
            categorical[
                torch.arange(num_atoms),
                atom_types[:num_atoms].clamp(min=0, max=num_atom_types - 1),
            ] = 1.0

        x_tensors.append(x_pad)
        cat_tensors.append(categorical)
        nodes_counts.append(num_atoms)

        if "group_index" in sample:
            group_indices.append(int(sample["group_index"]))
        else:
            has_group_index = False

        if "timesteps" in sample:
            ts = sample["timesteps"]
            if not isinstance(ts, torch.Tensor):
                ts = torch.tensor(ts)
            timesteps.append(ts.to(torch.float32))
        else:
            has_timesteps = False

    batch_size = len(x_tensors)
    batch = TensorDict(
        {
            "x": torch.stack(x_tensors, dim=0),
            "categorical": torch.stack(cat_tensors, dim=0),
            "nodesxsample": torch.tensor(nodes_counts, dtype=torch.long),
        },
        batch_size=[batch_size],
    )

    if has_group_index:
        batch["group_index"] = torch.tensor(group_indices, dtype=torch.long)
    if has_timesteps:
        lengths = {t.shape[0] for t in timesteps}
        if len(lengths) == 1:
            batch["timesteps"] = torch.stack(timesteps, dim=0)

    meta_info = {
        "max_n_nodes": max_n_nodes,
        "condition": False,
        "force_alignment_enabled": False,
        "share_initial_noise": False,
    }

    return DataProto(batch=batch, meta_info=meta_info)


def init_rewarder(
    dataset_info: Dict[str, Any],
    reward_cfg: Dict[str, Any],
    device: torch.device,
) -> UMAForceReward:
    reward_cfg = reward_cfg or {}
    shaping_cfg = reward_cfg.get("shaping", {}) if isinstance(reward_cfg, dict) else {}
    shaping_override = {"enabled": False, "skip_prefix": 0, "scheduler": {"skip_prefix": 0}}

    rewarder = UMAForceReward(
        dataset_info=dataset_info,
        condition=False,
        mlff_model=reward_cfg.get("mlff_model", "uma-s-1p1"),
        mlff_predictor=None,
        position_scale=None,
        force_clip_threshold=reward_cfg.get("force_clip_threshold"),
        device=device,
        mlff_device=device,
        shaping=shaping_override,
        use_energy=bool(reward_cfg.get("use_energy", False)),
        force_weight=float(reward_cfg.get("force_weight", 1.0)),
        energy_weight=float(reward_cfg.get("energy_weight", 1.0)),
        stability_weight=float(reward_cfg.get("stability_weight", 0.0)),
        force_aggregation=reward_cfg.get("force_aggregation", "rms"),
        energy_transform_offset=float(reward_cfg.get("energy_transform_offset", 10000.0)),
        energy_transform_scale=float(reward_cfg.get("energy_transform_scale", 1000.0)),
    )
    return rewarder


def append_metrics(target: Dict[str, List[torch.Tensor]], metrics: TensorDict) -> None:
    for key in metrics.keys():
        value = metrics[key]
        if isinstance(value, torch.Tensor):
            target.setdefault(key, []).append(value.detach().cpu())


def summarize_metrics(metric_store: Dict[str, List[torch.Tensor]]) -> Tuple[Dict[str, Any], Dict[str, torch.Tensor]]:
    aggregate: Dict[str, Any] = {}
    flattened: Dict[str, torch.Tensor] = {}
    total_samples = 0

    for key, tensors in metric_store.items():
        if not tensors:
            continue
        stacked = torch.cat(tensors, dim=0)
        flattened[key] = stacked
        total_samples = stacked.shape[0]
        mean = stacked.mean().item()
        if stacked.numel() > 1:
            std = stacked.std(unbiased=False).item()
        else:
            std = 0.0
        aggregate[key] = {
            "mean": mean,
            "std": std,
            "min": stacked.min().item(),
            "max": stacked.max().item(),
        }
        if key == "stability":
            aggregate["stability_rate"] = mean

    aggregate["num_samples"] = total_samples
    return aggregate, flattened


def main() -> None:
    args = parse_args()
    cwd = Path(os.getcwd())
    run_dir = make_absolute(args.run_dir, cwd) if not Path(args.run_dir).is_absolute() else Path(args.run_dir)
    run_dir = run_dir.resolve()

    if not run_dir.exists():
        raise FileNotFoundError(f"Run directory '{run_dir}' does not exist.")

    samples_path = (
        make_absolute(args.samples, run_dir) if args.samples else run_dir / "eval_rollouts.pt"
    )
    if not samples_path.exists():
        raise FileNotFoundError(f"Samples file '{samples_path}' not found.")

    args_pickle_path = (
        make_absolute(args.args_pickle, run_dir) if args.args_pickle else run_dir / "args.pickle"
    )
    if not args_pickle_path.exists():
        raise FileNotFoundError(f"args.pickle not found at '{args_pickle_path}'.")

    run_config = load_run_config(run_dir)
    reward_cfg = run_config.get("reward", {}) if isinstance(run_config, dict) else {}

    edm_config = load_edm_config(args_pickle_path)
    dataset_info = get_dataset_info(edm_config.dataset, edm_config.remove_h)

    device = select_device(args.device)
    rewarder = init_rewarder(dataset_info, reward_cfg, device)

    samples, sample_metadata = load_samples(samples_path)
    batch_size = max(1, int(args.batch_size))
    metric_store: Dict[str, List[torch.Tensor]] = {}

    num_batches = math.ceil(len(samples) / batch_size)
    for batch_idx in tqdm(range(num_batches), desc="Computing metrics", unit="batch"):
        start = batch_idx * batch_size
        end = min(len(samples), start + batch_size)
        batch_samples = samples[start:end]
        data_proto = build_dataproto(batch_samples, dataset_info)
        result_proto = rewarder.calculate_rewards(data_proto)
        append_metrics(metric_store, result_proto.batch)

    summary, flattened = summarize_metrics(metric_store)
    rdkit_metrics = sample_metadata.get("rdkit_metrics") if sample_metadata else None
    if not rdkit_metrics:
        rdkit_metrics = compute_rdkit_metrics(samples, dataset_info)
    summary["rdkit_metrics"] = rdkit_metrics

    print("UMA evaluation complete")
    print(f"Total samples: {summary.get('num_samples', 0)}")
    for key, stats in summary.items():
        if key in {"num_samples", "stability_rate", "rdkit_metrics"}:
            continue
        if isinstance(stats, dict) and "mean" in stats and "std" in stats:
            mean = stats["mean"]
            std = stats["std"]
            print(f"{key:>25}: mean={mean:.6f} std={std:.6f}")
    if "stability_rate" in summary:
        print(f"{'stability_rate':>25}: {summary['stability_rate']:.6f}")
    if rdkit_metrics:
        if "error" in rdkit_metrics:
            print(f"{'rdkit_metrics':>25}: {rdkit_metrics['error']}")
        else:
            validity_pct = rdkit_metrics["validity"] * 100.0
            uniqueness_pct = rdkit_metrics["uniqueness"] * 100.0
            num_total = rdkit_metrics["num_total"]
            num_valid = rdkit_metrics["num_valid"]
            num_unique = rdkit_metrics["num_unique"]
            print(
                f"{'rdkit_validity':>25}: {validity_pct:.2f}% ({num_valid}/{num_total})"
            )
            if num_valid > 0:
                print(
                    f"{'rdkit_uniqueness':>25}: {uniqueness_pct:.2f}% ({num_unique}/{num_valid} valid)"
                )
            else:
                print(f"{'rdkit_uniqueness':>25}: n/a (no valid molecules)")

    if args.output:
        output_path = make_absolute(args.output, run_dir) if not Path(args.output).is_absolute() else Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w") as f:
            json.dump(summary, f, indent=2)
        print(f"Saved aggregate metrics to {output_path}")

    if args.save_raw:
        raw_path = (
            make_absolute(args.save_raw, run_dir)
            if not Path(args.save_raw).is_absolute()
            else Path(args.save_raw)
        )
        raw_path.parent.mkdir(parents=True, exist_ok=True)
        torch.save(flattened, raw_path)
        print(f"Saved per-sample metrics to {raw_path}")


if __name__ == "__main__":
    main()
